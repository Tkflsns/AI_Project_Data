{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad9378a-1323-44a6-9432-09eb959a01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lincenseplateocr.utils import CTCLabelConverter, AttnLabelConverter\n",
    "from lincenseplateocr.dataset import RawDataset, AlignCollate\n",
    "from lincenseplateocr.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8f5ebb0-de18-44e1-9df4-bc0be5099e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd548eb-37ad-4a27-a363-ffa9f1bfa4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(opt):\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    print(\"Initializing converter...\")  # 디버깅 추가\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "    print(f\"Number of classes: {opt.num_class}\")\n",
    "\n",
    "    print(\"Setting input channels...\")  # 디버깅 추가\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "\n",
    "    print(\"Initializing model...\")  # 디버깅 추가\n",
    "    model = Model(opt)\n",
    "    print(\"Model initialized.\")  # 모델이 정상적으로 초기화되었는지 확인\n",
    "\n",
    "    # 데이터 병렬처리 설정 (문제가 발생할 수 있으므로 테스트할 필요가 있음)\n",
    "    print(\"Setting model to DataParallel...\")\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    print(\"Model set to DataParallel.\")\n",
    "\n",
    "    # 모델 가중치 로드\n",
    "    print(f\"Loading pretrained model from {opt.saved_model}...\")\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    # prepare data\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "    # predict\n",
    "    model.eval()\n",
    "    output_file_paths = []\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image, text_for_pred)\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, preds_size)\n",
    "            else:\n",
    "                preds = model(image, text_for_pred, is_train=False)\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}'\n",
    "            \n",
    "            print(f'{dashed_line}\\n{head}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                \n",
    "                # os.path.basename으로 파일 이름만 추출하여 output 폴더에 저장\n",
    "                file_name = os.path.basename(img_name)\n",
    "                \n",
    "                # output 디렉토리가 없으면 생성\n",
    "                if not os.path.exists(opt.output_folder):\n",
    "                    os.makedirs(opt.output_folder)\n",
    "\n",
    "                # output 폴더에 파일 작성\n",
    "                log_path = os.path.join(opt.output_folder, f'{file_name}.txt')\n",
    "                with open(log_path, 'a') as log:\n",
    "                    log.write(f'{head}\\n{dashed_line}\\n')\n",
    "                    print(f'{img_name:25s}\\t{pred:25s}\\n')\n",
    "                    log.write(f'{img_name:25s}\\t{pred:25s}\\n')\n",
    "\n",
    "                output_file_paths.append(log_path)\n",
    "\n",
    "    return output_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad51cf2-2836-4ec0-92b7-36e7a65fb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter 환경에서는 명령줄 인자를 수동으로 설정합니다.\n",
    "class Opt:\n",
    "    def __init__(self, image_folder, output_folder):\n",
    "        self.image_folder = image_folder  # 외부에서 입력받은 이미지 폴더 경로\n",
    "        self.output_folder = output_folder  # 외부에서 입력받은 출력 폴더 경로\n",
    "        self.workers = 0\n",
    "        self.batch_size = 32\n",
    "        self.saved_model = 'lincenseplateocr/pretrained/Fine-Tuned.pth'\n",
    "        self.batch_max_length = 16\n",
    "        self.imgH = 32\n",
    "        self.imgW = 100\n",
    "        self.rgb = False\n",
    "        self.character = '0123456789().JNRW_abcdef가강개걍거겅겨견결경계고과관광굥구금기김깅나남너노논누니다대댜더뎡도동두등디라러로루룰리마머명모무문므미바배뱌버베보부북비사산서성세셔소송수시아악안양어여연영오올용우울원육으을이익인자작저전제조종주중지차처천초추출충층카콜타파평포하허호홀후히ㅣ'\n",
    "        self.sensitive = False\n",
    "        self.PAD = False\n",
    "        self.Transformation = 'TPS'\n",
    "        self.FeatureExtraction = 'ResNet'\n",
    "        self.SequenceModeling = 'BiLSTM'\n",
    "        self.Prediction = 'Attn'\n",
    "        self.num_fiducial = 20\n",
    "        self.input_channel = 1\n",
    "        self.output_channel = 512\n",
    "        self.hidden_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c30409b-4f89-4b65-9b4a-0ae447c064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 설정한 옵션 객체 생성\n",
    "def process_images(image_folder, output_folder):\n",
    "    opt = Opt(image_folder, output_folder)\n",
    "\n",
    "    if opt.sensitive:\n",
    "        opt.character = string.printable[:-6]\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = True\n",
    "    opt.num_gpu = torch.cuda.device_count()\n",
    "\n",
    "    # 모델 실행 및 결과 텍스트 파일 경로 리턴\n",
    "    output_file_paths = demo(opt)\n",
    "    return output_file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd30f28f-bbe7-4248-8270-f81bbc331c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing converter...\n",
      "Number of classes: 148\n",
      "Setting input channels...\n",
      "Initializing model...\n",
      "Initializing TPS Transformation\n",
      "TPS 초기화: F=20, I_size=(32, 100), I_r_size=(32, 100), I_channel_num=1\n",
      "Localization Network 초기화 중...\n",
      "Localization Network 초기화: F=20, I_channel_num=1\n",
      "Localization Network Conv layers 초기화 중...\n",
      "Localization Network Conv layers 초기화 완료\n",
      "Localization Network Fully connected layers 초기화 중...\n",
      "Localization Network Fully connected layers 초기화 완료\n",
      "Localization Network 초기화 완료\n",
      "Grid Generator 초기화 중...\n",
      "Grid Generator 초기화 완료\n",
      "TPS Transformation initialized\n",
      "Initializing Feature Extraction: ResNet\n",
      "Feature Extraction initialized with output size: 512\n",
      "Initializing Sequence Modeling with BiLSTM\n",
      "Sequence Modeling initialized\n",
      "Initializing Prediction: Attn\n",
      "Prediction initialized\n",
      "Model initialized.\n",
      "Setting model to DataParallel...\n",
      "Model set to DataParallel.\n",
      "Loading pretrained model from lincenseplateocr/pretrained/Fine-Tuned.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darkh\\anaconda3\\envs\\torch-book\\Lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "C:\\Devtools\\project\\lincenseplateocr\\modules\\transformation.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
      "C:\\Devtools\\project\\lincenseplateocr\\modules\\transformation.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
      "C:\\Users\\darkh\\AppData\\Local\\Temp\\ipykernel_37780\\2057111695.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "입력 이미지 크기: torch.Size([30, 1, 32, 100])\n",
      "Localization Network 실행 중...\n",
      "예측된 fiducial points 크기: torch.Size([30, 20, 2])\n",
      "Grid 생성 중...\n",
      "생성된 그리드 크기: torch.Size([30, 32, 100, 2])\n",
      "grid_sample 이후 출력 크기: torch.Size([30, 1, 32, 100])\n",
      "--------------------------------------------------------------------------------\n",
      "image_path               \tpredicted_labels         \n",
      "\n",
      "lincenseplateocr/input\\1296 (4)_crop_0.jpg\t경기82사1256                \n",
      "\n",
      "lincenseplateocr/input\\1296 (4)_crop_0_out1.jpg\t인천84사1296                \n",
      "\n",
      "lincenseplateocr/input\\1379 (4)_crop_0.jpg\t8581339                  \n",
      "\n",
      "lincenseplateocr/input\\1379 (4)_crop_0_out1.jpg\t8831848                  \n",
      "\n",
      "lincenseplateocr/input\\1379 (5)_crop_0.jpg\t서울81자3379                \n",
      "\n",
      "lincenseplateocr/input\\1379 (5)_crop_0_out1.jpg\t서울81바1379                \n",
      "\n",
      "lincenseplateocr/input\\1459 (22)_crop_0.jpg\t인천81배1459                \n",
      "\n",
      "lincenseplateocr/input\\1459 (22)_crop_0_out1.jpg\t충북91아1459                \n",
      "\n",
      "lincenseplateocr/input\\1459 (43)_crop_0.jpg\t8북84배1459                \n",
      "\n",
      "lincenseplateocr/input\\1459 (43)_crop_0_out1.jpg\t인천85바1459                \n",
      "\n",
      "lincenseplateocr/input\\1459_crop_0.jpg\t서울33자1459                \n",
      "\n",
      "lincenseplateocr/input\\1459_crop_0_out1.jpg\t88가1589                  \n",
      "\n",
      "lincenseplateocr/input\\1547 (13)_crop_0.jpg\t인천서서마1542                \n",
      "\n",
      "lincenseplateocr/input\\1547 (13)_crop_0_out1.jpg\t전북81바1847                \n",
      "\n",
      "lincenseplateocr/input\\1602 (10)_crop_0.jpg\t8882사1602                \n",
      "\n",
      "lincenseplateocr/input\\1602 (10)_crop_0_out1.jpg\t78고8102                  \n",
      "\n",
      "lincenseplateocr/input\\1602 (30)_crop_0.jpg\t경기80다1602                \n",
      "\n",
      "lincenseplateocr/input\\1602 (30)_crop_0_out1.jpg\t인천82바1602                \n",
      "\n",
      "lincenseplateocr/input\\1892 (30)_crop_0.jpg\t서울88아1892                \n",
      "\n",
      "lincenseplateocr/input\\1892 (30)_crop_0_out1.jpg\t충북87아1892                \n",
      "\n",
      "lincenseplateocr/input\\2218 (15)_crop_0.jpg\t서울88바2218                \n",
      "\n",
      "lincenseplateocr/input\\2218 (15)_crop_0_out1.jpg\t인천88바2218                \n",
      "\n",
      "lincenseplateocr/input\\2799 (1)_crop_0.jpg\t서울85자2399                \n",
      "\n",
      "lincenseplateocr/input\\2799 (1)_crop_0_out1.jpg\t서울85사2799                \n",
      "\n",
      "lincenseplateocr/input\\2799 (40)_crop_0.jpg\t서울고1다2799                \n",
      "\n",
      "lincenseplateocr/input\\2799 (40)_crop_0_out1.jpg\t인천87아2799                \n",
      "\n",
      "lincenseplateocr/input\\3214 (5)_crop_0.jpg\t인천81바3214                \n",
      "\n",
      "lincenseplateocr/input\\3214 (5)_crop_0_out1.jpg\t서울서1자3214                \n",
      "\n",
      "lincenseplateocr/input\\3319 (5)_crop_0.jpg\t8883아31                  \n",
      "\n",
      "lincenseplateocr/input\\3319 (5)_crop_0_out1.jpg\t경기84자6348                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 예시: 외부에서 파일 경로를 받아 처리\n",
    "image_folder = 'path_to_image_folder'  # 실제 이미지 폴더 경로\n",
    "output_folder = 'path_to_output_folder'  # 실제 출력 폴더 경로\n",
    "result_file_paths = process_images(image_folder, output_folder)\n",
    "print(f\"Result files saved at: {result_file_paths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68134042-737a-424a-a456-46465810ae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
