{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "60e052ca-b131-4bd8-9966-303c7ce4a269",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 14,
   "id": "60e052ca-b131-4bd8-9966-303c7ce4a269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using converter: <class 'lincenseplateocr.utils.AttnLabelConverter'>\n",
      "Initializing TPS Transformation\n",
      "TPS 초기화: F=20, I_size=(32, 100), I_r_size=(32, 100), I_channel_num=1\n",
      "TPS Transformation initialized\n",
      "Initializing Feature Extraction: ResNet\n",
      "Feature Extraction initialized with output size: 512\n",
      "Initializing Sequence Modeling with BiLSTM\n",
      "Sequence Modeling initialized\n",
      "Initializing Prediction: Attn\n",
      "Prediction initialized\n",
      "Fine-tuning mode: Loading pretrained model for fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20716\\1035989257.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  str_model.load_state_dict(copyStateDict(torch.load(opt.saved_model)), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction.attention_cell.i2h.weight - requires_grad: True\n",
      "Prediction.attention_cell.h2h.weight - requires_grad: True\n",
      "Prediction.attention_cell.h2h.bias - requires_grad: True\n",
      "Prediction.attention_cell.score.weight - requires_grad: True\n",
      "Prediction.attention_cell.rnn.weight_ih - requires_grad: True\n",
      "Prediction.attention_cell.rnn.weight_hh - requires_grad: True\n",
      "Prediction.attention_cell.rnn.bias_ih - requires_grad: True\n",
      "Prediction.attention_cell.rnn.bias_hh - requires_grad: True\n",
      "Prediction.generator.weight - requires_grad: True\n",
      "Prediction.generator.bias - requires_grad: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20716\\1035989257.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  refiner_net.load_state_dict(copyStateDict(torch.load(opt.refiner_model, map_location=device)))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CRAFT:\n\tMissing key(s) in state_dict: \"basenet.slice1.0.weight\", \"basenet.slice1.0.bias\", \"basenet.slice1.1.weight\", \"basenet.slice1.1.bias\", \"basenet.slice1.1.running_mean\", \"basenet.slice1.1.running_var\", \"basenet.slice1.3.weight\", \"basenet.slice1.3.bias\", \"basenet.slice1.4.weight\", \"basenet.slice1.4.bias\", \"basenet.slice1.4.running_mean\", \"basenet.slice1.4.running_var\", \"basenet.slice1.7.weight\", \"basenet.slice1.7.bias\", \"basenet.slice1.8.weight\", \"basenet.slice1.8.bias\", \"basenet.slice1.8.running_mean\", \"basenet.slice1.8.running_var\", \"basenet.slice1.10.weight\", \"basenet.slice1.10.bias\", \"basenet.slice1.11.weight\", \"basenet.slice1.11.bias\", \"basenet.slice1.11.running_mean\", \"basenet.slice1.11.running_var\", \"basenet.slice2.14.weight\", \"basenet.slice2.14.bias\", \"basenet.slice2.15.weight\", \"basenet.slice2.15.bias\", \"basenet.slice2.15.running_mean\", \"basenet.slice2.15.running_var\", \"basenet.slice2.17.weight\", \"basenet.slice2.17.bias\", \"basenet.slice2.18.weight\", \"basenet.slice2.18.bias\", \"basenet.slice2.18.running_mean\", \"basenet.slice2.18.running_var\", \"basenet.slice3.20.weight\", \"basenet.slice3.20.bias\", \"basenet.slice3.21.weight\", \"basenet.slice3.21.bias\", \"basenet.slice3.21.running_mean\", \"basenet.slice3.21.running_var\", \"basenet.slice3.24.weight\", \"basenet.slice3.24.bias\", \"basenet.slice3.25.weight\", \"basenet.slice3.25.bias\", \"basenet.slice3.25.running_mean\", \"basenet.slice3.25.running_var\", \"basenet.slice3.27.weight\", \"basenet.slice3.27.bias\", \"basenet.slice3.28.weight\", \"basenet.slice3.28.bias\", \"basenet.slice3.28.running_mean\", \"basenet.slice3.28.running_var\", \"basenet.slice4.30.weight\", \"basenet.slice4.30.bias\", \"basenet.slice4.31.weight\", \"basenet.slice4.31.bias\", \"basenet.slice4.31.running_mean\", \"basenet.slice4.31.running_var\", \"basenet.slice4.34.weight\", \"basenet.slice4.34.bias\", \"basenet.slice4.35.weight\", \"basenet.slice4.35.bias\", \"basenet.slice4.35.running_mean\", \"basenet.slice4.35.running_var\", \"basenet.slice4.37.weight\", \"basenet.slice4.37.bias\", \"basenet.slice4.38.weight\", \"basenet.slice4.38.bias\", \"basenet.slice4.38.running_mean\", \"basenet.slice4.38.running_var\", \"basenet.slice5.1.weight\", \"basenet.slice5.1.bias\", \"basenet.slice5.2.weight\", \"basenet.slice5.2.bias\", \"upconv1.conv.0.weight\", \"upconv1.conv.0.bias\", \"upconv1.conv.1.weight\", \"upconv1.conv.1.bias\", \"upconv1.conv.1.running_mean\", \"upconv1.conv.1.running_var\", \"upconv1.conv.3.weight\", \"upconv1.conv.3.bias\", \"upconv1.conv.4.weight\", \"upconv1.conv.4.bias\", \"upconv1.conv.4.running_mean\", \"upconv1.conv.4.running_var\", \"upconv2.conv.0.weight\", \"upconv2.conv.0.bias\", \"upconv2.conv.1.weight\", \"upconv2.conv.1.bias\", \"upconv2.conv.1.running_mean\", \"upconv2.conv.1.running_var\", \"upconv2.conv.3.weight\", \"upconv2.conv.3.bias\", \"upconv2.conv.4.weight\", \"upconv2.conv.4.bias\", \"upconv2.conv.4.running_mean\", \"upconv2.conv.4.running_var\", \"upconv3.conv.0.weight\", \"upconv3.conv.0.bias\", \"upconv3.conv.1.weight\", \"upconv3.conv.1.bias\", \"upconv3.conv.1.running_mean\", \"upconv3.conv.1.running_var\", \"upconv3.conv.3.weight\", \"upconv3.conv.3.bias\", \"upconv3.conv.4.weight\", \"upconv3.conv.4.bias\", \"upconv3.conv.4.running_mean\", \"upconv3.conv.4.running_var\", \"upconv4.conv.0.weight\", \"upconv4.conv.0.bias\", \"upconv4.conv.1.weight\", \"upconv4.conv.1.bias\", \"upconv4.conv.1.running_mean\", \"upconv4.conv.1.running_var\", \"upconv4.conv.3.weight\", \"upconv4.conv.3.bias\", \"upconv4.conv.4.weight\", \"upconv4.conv.4.bias\", \"upconv4.conv.4.running_mean\", \"upconv4.conv.4.running_var\", \"conv_cls.0.weight\", \"conv_cls.0.bias\", \"conv_cls.2.weight\", \"conv_cls.2.bias\", \"conv_cls.4.weight\", \"conv_cls.4.bias\", \"conv_cls.6.weight\", \"conv_cls.6.bias\", \"conv_cls.8.weight\", \"conv_cls.8.bias\". \n\tUnexpected key(s) in state_dict: \"last_conv.0.weight\", \"last_conv.0.bias\", \"last_conv.1.weight\", \"last_conv.1.bias\", \"last_conv.1.running_mean\", \"last_conv.1.running_var\", \"last_conv.3.weight\", \"last_conv.3.bias\", \"last_conv.4.weight\", \"last_conv.4.bias\", \"last_conv.4.running_mean\", \"last_conv.4.running_var\", \"last_conv.6.weight\", \"last_conv.6.bias\", \"last_conv.7.weight\", \"last_conv.7.bias\", \"last_conv.7.running_mean\", \"last_conv.7.running_var\", \"aspp1.0.weight\", \"aspp1.0.bias\", \"aspp1.1.weight\", \"aspp1.1.bias\", \"aspp1.1.running_mean\", \"aspp1.1.running_var\", \"aspp1.3.weight\", \"aspp1.3.bias\", \"aspp1.4.weight\", \"aspp1.4.bias\", \"aspp1.4.running_mean\", \"aspp1.4.running_var\", \"aspp1.6.weight\", \"aspp1.6.bias\", \"aspp2.0.weight\", \"aspp2.0.bias\", \"aspp2.1.weight\", \"aspp2.1.bias\", \"aspp2.1.running_mean\", \"aspp2.1.running_var\", \"aspp2.3.weight\", \"aspp2.3.bias\", \"aspp2.4.weight\", \"aspp2.4.bias\", \"aspp2.4.running_mean\", \"aspp2.4.running_var\", \"aspp2.6.weight\", \"aspp2.6.bias\", \"aspp3.0.weight\", \"aspp3.0.bias\", \"aspp3.1.weight\", \"aspp3.1.bias\", \"aspp3.1.running_mean\", \"aspp3.1.running_var\", \"aspp3.3.weight\", \"aspp3.3.bias\", \"aspp3.4.weight\", \"aspp3.4.bias\", \"aspp3.4.running_mean\", \"aspp3.4.running_var\", \"aspp3.6.weight\", \"aspp3.6.bias\", \"aspp4.0.weight\", \"aspp4.0.bias\", \"aspp4.1.weight\", \"aspp4.1.bias\", \"aspp4.1.running_mean\", \"aspp4.1.running_var\", \"aspp4.3.weight\", \"aspp4.3.bias\", \"aspp4.4.weight\", \"aspp4.4.bias\", \"aspp4.4.running_mean\", \"aspp4.4.running_var\", \"aspp4.6.weight\", \"aspp4.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 371\u001b[0m\n\u001b[0;32m    367\u001b[0m     str_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# 트레이닝 실행\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 284\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m이미지 불러오기 오류: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - 파일 경로: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 라벨 경로 : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m boxes, polys \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_text_craft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcraft_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 파라미터 전달\u001b[39;00m\n\u001b[0;32m    286\u001b[0m  \u001b[38;5;66;03m# 원래 이미지에 박스를 그림\u001b[39;00m\n\u001b[0;32m    287\u001b[0m image_with_boxes \u001b[38;5;241m=\u001b[39m draw_boxes_on_image(image, boxes)\n",
      "Cell \u001b[1;32mIn[14], line 212\u001b[0m, in \u001b[0;36mdetect_text_craft\u001b[1;34m(image, craft_net, opt)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mrefine:\n\u001b[0;32m    211\u001b[0m     refiner_net \u001b[38;5;241m=\u001b[39m CRAFT()  \u001b[38;5;66;03m# 리파이너 모델 로드\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     \u001b[43mrefiner_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopyStateDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefiner_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m     refiner_net \u001b[38;5;241m=\u001b[39m refiner_net\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    214\u001b[0m     refiner_net\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CRAFT:\n\tMissing key(s) in state_dict: \"basenet.slice1.0.weight\", \"basenet.slice1.0.bias\", \"basenet.slice1.1.weight\", \"basenet.slice1.1.bias\", \"basenet.slice1.1.running_mean\", \"basenet.slice1.1.running_var\", \"basenet.slice1.3.weight\", \"basenet.slice1.3.bias\", \"basenet.slice1.4.weight\", \"basenet.slice1.4.bias\", \"basenet.slice1.4.running_mean\", \"basenet.slice1.4.running_var\", \"basenet.slice1.7.weight\", \"basenet.slice1.7.bias\", \"basenet.slice1.8.weight\", \"basenet.slice1.8.bias\", \"basenet.slice1.8.running_mean\", \"basenet.slice1.8.running_var\", \"basenet.slice1.10.weight\", \"basenet.slice1.10.bias\", \"basenet.slice1.11.weight\", \"basenet.slice1.11.bias\", \"basenet.slice1.11.running_mean\", \"basenet.slice1.11.running_var\", \"basenet.slice2.14.weight\", \"basenet.slice2.14.bias\", \"basenet.slice2.15.weight\", \"basenet.slice2.15.bias\", \"basenet.slice2.15.running_mean\", \"basenet.slice2.15.running_var\", \"basenet.slice2.17.weight\", \"basenet.slice2.17.bias\", \"basenet.slice2.18.weight\", \"basenet.slice2.18.bias\", \"basenet.slice2.18.running_mean\", \"basenet.slice2.18.running_var\", \"basenet.slice3.20.weight\", \"basenet.slice3.20.bias\", \"basenet.slice3.21.weight\", \"basenet.slice3.21.bias\", \"basenet.slice3.21.running_mean\", \"basenet.slice3.21.running_var\", \"basenet.slice3.24.weight\", \"basenet.slice3.24.bias\", \"basenet.slice3.25.weight\", \"basenet.slice3.25.bias\", \"basenet.slice3.25.running_mean\", \"basenet.slice3.25.running_var\", \"basenet.slice3.27.weight\", \"basenet.slice3.27.bias\", \"basenet.slice3.28.weight\", \"basenet.slice3.28.bias\", \"basenet.slice3.28.running_mean\", \"basenet.slice3.28.running_var\", \"basenet.slice4.30.weight\", \"basenet.slice4.30.bias\", \"basenet.slice4.31.weight\", \"basenet.slice4.31.bias\", \"basenet.slice4.31.running_mean\", \"basenet.slice4.31.running_var\", \"basenet.slice4.34.weight\", \"basenet.slice4.34.bias\", \"basenet.slice4.35.weight\", \"basenet.slice4.35.bias\", \"basenet.slice4.35.running_mean\", \"basenet.slice4.35.running_var\", \"basenet.slice4.37.weight\", \"basenet.slice4.37.bias\", \"basenet.slice4.38.weight\", \"basenet.slice4.38.bias\", \"basenet.slice4.38.running_mean\", \"basenet.slice4.38.running_var\", \"basenet.slice5.1.weight\", \"basenet.slice5.1.bias\", \"basenet.slice5.2.weight\", \"basenet.slice5.2.bias\", \"upconv1.conv.0.weight\", \"upconv1.conv.0.bias\", \"upconv1.conv.1.weight\", \"upconv1.conv.1.bias\", \"upconv1.conv.1.running_mean\", \"upconv1.conv.1.running_var\", \"upconv1.conv.3.weight\", \"upconv1.conv.3.bias\", \"upconv1.conv.4.weight\", \"upconv1.conv.4.bias\", \"upconv1.conv.4.running_mean\", \"upconv1.conv.4.running_var\", \"upconv2.conv.0.weight\", \"upconv2.conv.0.bias\", \"upconv2.conv.1.weight\", \"upconv2.conv.1.bias\", \"upconv2.conv.1.running_mean\", \"upconv2.conv.1.running_var\", \"upconv2.conv.3.weight\", \"upconv2.conv.3.bias\", \"upconv2.conv.4.weight\", \"upconv2.conv.4.bias\", \"upconv2.conv.4.running_mean\", \"upconv2.conv.4.running_var\", \"upconv3.conv.0.weight\", \"upconv3.conv.0.bias\", \"upconv3.conv.1.weight\", \"upconv3.conv.1.bias\", \"upconv3.conv.1.running_mean\", \"upconv3.conv.1.running_var\", \"upconv3.conv.3.weight\", \"upconv3.conv.3.bias\", \"upconv3.conv.4.weight\", \"upconv3.conv.4.bias\", \"upconv3.conv.4.running_mean\", \"upconv3.conv.4.running_var\", \"upconv4.conv.0.weight\", \"upconv4.conv.0.bias\", \"upconv4.conv.1.weight\", \"upconv4.conv.1.bias\", \"upconv4.conv.1.running_mean\", \"upconv4.conv.1.running_var\", \"upconv4.conv.3.weight\", \"upconv4.conv.3.bias\", \"upconv4.conv.4.weight\", \"upconv4.conv.4.bias\", \"upconv4.conv.4.running_mean\", \"upconv4.conv.4.running_var\", \"conv_cls.0.weight\", \"conv_cls.0.bias\", \"conv_cls.2.weight\", \"conv_cls.2.bias\", \"conv_cls.4.weight\", \"conv_cls.4.bias\", \"conv_cls.6.weight\", \"conv_cls.6.bias\", \"conv_cls.8.weight\", \"conv_cls.8.bias\". \n\tUnexpected key(s) in state_dict: \"last_conv.0.weight\", \"last_conv.0.bias\", \"last_conv.1.weight\", \"last_conv.1.bias\", \"last_conv.1.running_mean\", \"last_conv.1.running_var\", \"last_conv.3.weight\", \"last_conv.3.bias\", \"last_conv.4.weight\", \"last_conv.4.bias\", \"last_conv.4.running_mean\", \"last_conv.4.running_var\", \"last_conv.6.weight\", \"last_conv.6.bias\", \"last_conv.7.weight\", \"last_conv.7.bias\", \"last_conv.7.running_mean\", \"last_conv.7.running_var\", \"aspp1.0.weight\", \"aspp1.0.bias\", \"aspp1.1.weight\", \"aspp1.1.bias\", \"aspp1.1.running_mean\", \"aspp1.1.running_var\", \"aspp1.3.weight\", \"aspp1.3.bias\", \"aspp1.4.weight\", \"aspp1.4.bias\", \"aspp1.4.running_mean\", \"aspp1.4.running_var\", \"aspp1.6.weight\", \"aspp1.6.bias\", \"aspp2.0.weight\", \"aspp2.0.bias\", \"aspp2.1.weight\", \"aspp2.1.bias\", \"aspp2.1.running_mean\", \"aspp2.1.running_var\", \"aspp2.3.weight\", \"aspp2.3.bias\", \"aspp2.4.weight\", \"aspp2.4.bias\", \"aspp2.4.running_mean\", \"aspp2.4.running_var\", \"aspp2.6.weight\", \"aspp2.6.bias\", \"aspp3.0.weight\", \"aspp3.0.bias\", \"aspp3.1.weight\", \"aspp3.1.bias\", \"aspp3.1.running_mean\", \"aspp3.1.running_var\", \"aspp3.3.weight\", \"aspp3.3.bias\", \"aspp3.4.weight\", \"aspp3.4.bias\", \"aspp3.4.running_mean\", \"aspp3.4.running_var\", \"aspp3.6.weight\", \"aspp3.6.bias\", \"aspp4.0.weight\", \"aspp4.0.bias\", \"aspp4.1.weight\", \"aspp4.1.bias\", \"aspp4.1.running_mean\", \"aspp4.1.running_var\", \"aspp4.3.weight\", \"aspp4.3.bias\", \"aspp4.4.weight\", \"aspp4.4.bias\", \"aspp4.4.running_mean\", \"aspp4.4.running_var\", \"aspp4.6.weight\", \"aspp4.6.bias\". "
     ]
    }
   ],
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.optim as optim\n",
<<<<<<< HEAD
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageDraw\n",
    "import numpy as np\n",
    "from CRAFT.craft import CRAFT\n",
    "import CRAFT.craft_utils\n",
    "import CRAFT.imgproc\n",
    "from lincenseplateocr.model import Model\n",
    "from lincenseplateocr.utils import AttnLabelConverter, CTCLabelConverter\n",
    "from lincenseplateocr.dataset import AlignCollate\n",
    "from lincenseplateocr.test import validation\n",
    "\n",
=======
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageEnhance, ImageDraw\n",
    "import CRAFT.craft_utils as craft_utils\n",
    "import CRAFT.imgproc as imgproc\n",
    "from CRAFT.craft import CRAFT\n",
    "from lincenseplateocr.model import Model\n",
    "from lincenseplateocr.utils import AttnLabelConverter, CTCLabelConverter\n",
    "from lincenseplateocr.dataset import AlignCollate, hierarchical_dataset\n",
    "from lincenseplateocr.test import validation\n",
    "from CRAFT.file_utils import saveResult\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Options:\n",
    "    def __init__(self):\n",
<<<<<<< HEAD
    "        self.exp_name = 'license_plate_training'  # 실험 이름\n",
    "        self.train_data = 'lincenseplateocr/input/train'  # 트레이닝 데이터 경로\n",
    "        self.valid_data = 'lincenseplateocr/input/vali'  # 검증 데이터 경로\n",
    "        self.saved_model = 'lincenseplateocr/pretrained/Fine-Tuned.pth'  # STR 모델의 사전 학습된 모델\n",
    "        self.num_iter = 3000  # 학습 반복 횟수\n",
    "        self.valInterval = 200  # 검증 간격\n",
    "        self.batch_size = 1  # 배치 크기\n",
    "        self.lr = 0.001  # 학습률\n",
    "        self.Prediction = 'CTC'  # STR의 예측 모드\n",
    "        self.batch_max_length = 25  # 최대 라벨 길이\n",
    "        self.imgH = 32  # 입력 이미지 높이\n",
    "        self.imgW = 100  # 입력 이미지 너비\n",
    "        self.character = '0123456789가나다라'  # 학습할 문자\n",
    "        self.input_channel = 1  # 입력 채널 (흑백 이미지)\n",
    "        self.output_channel = 512  # 출력 채널 수\n",
    "        self.hidden_size = 256  # LSTM 히든 사이즈\n",
    "        self.trained_craft_model = 'CRAFT/weights/craft_mlt_25k.pth'  # CRAFT 사전 학습된 가중치\n",
    "        self.workers = 0  # 데이터 로더 워커 수\n",
    "\n",
    "opt = Options()\n",
    "\n",
    "# CRAFT 모델 로드\n",
    "craft_net = CRAFT()\n",
    "craft_net.load_state_dict(torch.load(opt.trained_craft_model))\n",
=======
    "        self.exp_name = 'license_plate_training'\n",
    "        self.train_data = 'lincenseplateocr/input/train'\n",
    "        self.valid_data = 'lincenseplateocr/input/vali'\n",
    "        self.saved_model = 'lincenseplateocr/pretrained/Fine-Tuned.pth'\n",
    "        self.num_iter = 3000\n",
    "        self.valInterval = 1\n",
    "        self.batch_size = 1000\n",
    "        torch.cuda.empty_cache()\n",
    "        self.lr = 0.001\n",
    "        self.beta1 = 0.9\n",
    "        self.eps = 1e-8\n",
    "        self.Prediction = 'Attn'\n",
    "        self.batch_max_length = 10\n",
    "        self.imgH = 32\n",
    "        self.imgW = 100\n",
    "        self.character = '0123456789().JNRW_abcdef가강개걍거겅겨견결경계고과관광굥구금기김깅나남너노논누니다대댜더뎡도동두등디라러로루룰리마머명모무문므미바배뱌버베보부북비사산서성세셔소송수시아악안양어여연영오올용우울원육으을이익인자작저전제조종주중지차처천초추출충층카콜타파평포하허호홀후히ㅣ'\n",
    "        self.input_channel = 1\n",
    "        self.output_channel = 512\n",
    "        self.hidden_size = 256\n",
    "        self.workers = 0\n",
    "        self.FT = True\n",
    "        self.Transformation = 'TPS'\n",
    "        self.FeatureExtraction = 'ResNet'\n",
    "        self.SequenceModeling = 'BiLSTM'\n",
    "        self.num_fiducial = 20\n",
    "        self.data_filtering_off = False\n",
    "        self.rgb = False\n",
    "        self.sensitive = False\n",
    "\n",
    "        # New parameters for CRAFT\n",
    "        self.text_threshold = 0.8\n",
    "        self.low_text = 0.6\n",
    "        self.link_threshold = 0.68\n",
    "        self.use_cuda = True\n",
    "        self.canvas_size = 1280\n",
    "        self.mag_ratio = 2.0\n",
    "        self.poly = False\n",
    "        self.refine = False\n",
    "        self.trained_craft_model = 'CRAFT/weights/craft_mlt_25k.pth'\n",
    "        self.refiner_model = 'CRAFT/weights/craft_refiner_CTW1500.pth'\n",
    "        self.show_time = True\n",
    "\n",
    "opt = Options()\n",
    "\n",
    "# StateDict 복사 함수\n",
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "# CRAFT 모델 로드\n",
    "craft_net = CRAFT()\n",
    "craft_net.load_state_dict(copyStateDict(torch.load(opt.trained_craft_model, weights_only=True)))\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "craft_net = craft_net.to(device)\n",
    "craft_net.eval()\n",
    "\n",
    "# STR 모델 준비\n",
<<<<<<< HEAD
    "converter = AttnLabelConverter(opt.character) if 'CTC' in opt.Prediction else CTCLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "str_model = Model(opt).to(device)\n",
    "str_model.load_state_dict(torch.load(opt.saved_model))\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "optimizer = optim.Adam(str_model.parameters(), lr=opt.lr)\n",
    "\n",
    "\n",
=======
    "if opt.Prediction == 'Attn':\n",
    "    converter = AttnLabelConverter(opt.character)\n",
    "else:\n",
    "    converter = CTCLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "print(f\"Using converter: {type(converter)}\")\n",
    "\n",
    "# 모델 로드 부분 수정\n",
    "str_model = Model(opt).to(device)\n",
    "\n",
    "if opt.saved_model != '':\n",
    "    # Fine-tuning 옵션에 따른 requires_grad 설정\n",
    "    if opt.FT:\n",
    "        print(f'Fine-tuning mode: Loading pretrained model for fine-tuning')\n",
    "        str_model.load_state_dict(copyStateDict(torch.load(opt.saved_model)), strict=False)\n",
    "    \n",
    "        # Prediction 레이어 내 모든 파라미터의 requires_grad를 True로 설정\n",
    "        for name, param in str_model.named_parameters():\n",
    "            if 'Prediction' in name:  # Prediction 관련 파라미터만 True로 설정\n",
    "                param.requires_grad = True\n",
    "                print(f'{name} - requires_grad: True')\n",
    "            else:\n",
    "                param.requires_grad = False  # 나머지 레이어는 고정\n",
    "    else:\n",
    "        print(f'Loading pretrained model from {opt.saved_model}')\n",
    "        str_model.load_state_dict(copyStateDict(torch.load(opt.saved_model)))\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "filtered_parameters = []\n",
    "for p in filter(lambda p: p.requires_grad, str_model.parameters()):\n",
    "    filtered_parameters.append(p)\n",
    "\n",
    "# 학습할 파라미터가 없을 경우 오류가 발생하므로 이를 방지\n",
    "if len(filtered_parameters) == 0:\n",
    "    raise ValueError(\"No parameters available for training. Please check requires_grad settings.\")\n",
    "\n",
    "optimizer = optim.Adam(filtered_parameters, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "# DataAugmentation 클래스 정의 추가\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "class DataAugmentation:\n",
    "    \"\"\"데이터 증강 클래스: 이미지 손상, 저해상도, 각도 조정, 이미지 일부 가리기\"\"\"\n",
    "    \n",
    "    def __init__(self, imgH, imgW):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((imgH, imgW)),  # 기본 이미지 리사이즈\n",
    "            transforms.RandomApply([self.add_noise()], p=0.3),  # 50% 확률로 노이즈 추가\n",
    "            transforms.RandomApply([self.reduce_resolution()], p=0.3),  # 50% 확률로 해상도 저하\n",
    "            transforms.RandomApply([transforms.RandomRotation(degrees=(-15, 15))], p=0.3),  # 50% 확률로 각도 조정\n",
    "            transforms.RandomApply([self.occlude_image()], p=0.3),  # 50% 확률로 이미지 일부 가리기\n",
    "            transforms.ToTensor()  # 텐서로 변환\n",
    "        ])\n",
    "\n",
    "    def add_noise(self):\n",
    "        \"\"\"이미지에 랜덤 노이즈 추가\"\"\"\n",
    "        def noise(img):\n",
    "            np_img = np.array(img)\n",
<<<<<<< HEAD
    "            row, col, ch = np_img.shape\n",
    "            mean = 0\n",
    "            sigma = 10  # 노이즈 강도\n",
    "            gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "            gauss = gauss.reshape(row, col, ch)\n",
    "            noisy = np_img + gauss\n",
    "            noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "            return Image.fromarray(noisy)\n",
=======
    "    \n",
    "            if len(np_img.shape) == 2:  # 이미지가 2D일 경우\n",
    "                row, col = np_img.shape\n",
    "                ch = 1\n",
    "            else:\n",
    "                row, col, ch = np_img.shape\n",
    "        \n",
    "            mean = 0\n",
    "            sigma = 10  # 노이즈 강도\n",
    "            gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
    "            \n",
    "            if ch == 1:  # 흑백 이미지일 경우\n",
    "                gauss = gauss[:, :, 0]\n",
    "            \n",
    "            noisy = np_img + gauss\n",
    "            noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            if ch == 1:  # 흑백 이미지일 경우 모드를 'L'로 설정\n",
    "                return Image.fromarray(noisy, mode='L')\n",
    "            else:\n",
    "                return Image.fromarray(noisy)  # 컬러 이미지일 경우\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "        return transforms.Lambda(noise)\n",
    "\n",
    "    def reduce_resolution(self):\n",
    "        \"\"\"이미지의 해상도를 낮춤\"\"\"\n",
    "        def low_res(img):\n",
    "            # 현재 이미지 크기에서 다운샘플링 후 다시 업샘플링\n",
    "            small_img = img.resize((self.imgW // 4, self.imgH // 4), Image.BILINEAR)\n",
    "            return small_img.resize((self.imgW, self.imgH), Image.BILINEAR)\n",
    "        return transforms.Lambda(low_res)\n",
    "\n",
    "    def occlude_image(self):\n",
    "        \"\"\"이미지의 일부를 가림\"\"\"\n",
    "        def occlude(img):\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            # 이미지의 일부를 임의의 사각형으로 가림\n",
    "            w, h = img.size\n",
    "            x1, y1 = random.randint(0, w // 2), random.randint(0, h // 2)\n",
    "            x2, y2 = random.randint(x1 + w // 4, w), random.randint(y1 + h // 4, h)\n",
<<<<<<< HEAD
    "            draw.rectangle([x1, y1, x2, y2], fill=(0, 0, 0))  # 검정색으로 가림\n",
=======
    "            draw.rectangle([x1, y1, x2, y2], fill=0)  # 검정색으로 가림\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "            return img\n",
    "        return transforms.Lambda(occlude)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"이미지에 변환 적용\"\"\"\n",
    "        return self.transform(img)\n",
<<<<<<< HEAD
    "\n",
    "\n",
    "def detect_text_craft(image, craft_net):\n",
    "    \"\"\"CRAFT로 텍스트 영역 탐지\"\"\"\n",
    "    img_resized, target_ratio, _ = imgproc.resize_aspect_ratio(image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)\n",
=======
    "        \n",
    "def detect_text_craft(image, craft_net, opt):\n",
    "    \"\"\"CRAFT로 텍스트 영역 탐지\"\"\"\n",
    "    img_resized, target_ratio, _ = imgproc.resize_aspect_ratio(image, opt.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=opt.mag_ratio)\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    x = torch.from_numpy(x).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y, _ = craft_net(x)\n",
    "\n",
    "    score_text = y[0, :, :, 0].cpu().data.numpy()\n",
    "    score_link = y[0, :, :, 1].cpu().data.numpy()\n",
    "\n",
<<<<<<< HEAD
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold=0.7, link_threshold=0.4, low_text=0.4, poly=False)\n",
    "    return boxes\n",
    "\n",
=======
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold=opt.text_threshold, link_threshold=opt.link_threshold, low_text=opt.low_text, poly=opt.poly)\n",
    "    \n",
    "    if opt.refine:\n",
    "        refiner_net = CRAFT()  # 리파이너 모델 로드\n",
    "        refiner_net.load_state_dict(copyStateDict(torch.load(opt.refiner_model, map_location=device)))\n",
    "        refiner_net = refiner_net.to(device)\n",
    "        refiner_net.eval()\n",
    "        with torch.no_grad():\n",
    "            y_refiner = refiner_net(y)\n",
    "        score_link_refined = y_refiner[0, :, :, 0].cpu().data.numpy()\n",
    "        boxes, polys = craft_utils.refineDetBoxes(polys, score_link_refined, opt.text_threshold, opt.link_threshold, opt.low_text)\n",
    "    \n",
    "    return boxes, polys\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "\n",
    "def crop_text_regions(image, boxes):\n",
    "    \"\"\"탐지된 텍스트 영역을 자르기\"\"\"\n",
    "    cropped_images = []\n",
    "    for box in boxes:\n",
    "        poly = np.array(box).astype(np.int32).reshape((-1, 2))\n",
    "        x_min = np.min(poly[:, 0])\n",
    "        y_min = np.min(poly[:, 1])\n",
    "        x_max = np.max(poly[:, 0])\n",
    "        y_max = np.max(poly[:, 1])\n",
    "\n",
    "        cropped_img = image[y_min:y_max, x_min:x_max]\n",
    "        cropped_images.append(cropped_img)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
<<<<<<< HEAD
=======
    "def combine_text_predictions(preds_str):\n",
    "    \"\"\"탐지된 텍스트들을 하나의 문자열로 합침\"\"\"\n",
    "    combined_text = ''.join([''.join(pred) for pred in preds_str])  # 리스트를 문자열로 변환 후 합침\n",
    "    return combined_text\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "\n",
    "def load_label_json(label_path):\n",
    "    \"\"\"라벨 JSON 파일을 로드\"\"\"\n",
    "    with open(label_path, 'r', encoding='utf-8') as f:\n",
    "        label_data = json.load(f)\n",
    "    return label_data['value']  # 이미지의 실제 라벨\n",
    "\n",
<<<<<<< HEAD
    "\n",
    "def train(opt):\n",
    "    \"\"\"STR 모델 학습 루프\"\"\"\n",
    "    data_augmentation = DataAugmentation(opt.imgH, opt.imgW)  # 데이터 증강\n",
    "\n",
    "    for iteration in range(opt.num_iter):\n",
    "        total_loss = 0\n",
    "        # 학습용 데이터 폴더에서 파일 로드\n",
    "        for img_file in os.listdir(opt.train_data):\n",
    "            if img_file.endswith('.jpg'):  # 이미지 파일만 처리\n",
    "                image_path = os.path.join(opt.train_data, img_file)\n",
    "                label_path = os.path.splitext(image_path)[0] + \".json\"\n",
    "\n",
    "                # 이미지 및 라벨 로드\n",
    "                image = imgproc.loadImage(image_path)\n",
    "                label = load_label_json(label_path)\n",
    "\n",
    "                # CRAFT로 텍스트 영역 탐지\n",
    "                boxes = detect_text_craft(image, craft_net)\n",
    "                cropped_images = crop_text_regions(image, boxes)\n",
    "\n",
    "                # 각 자른 이미지 영역을 STR로 학습\n",
    "                for cropped_image in cropped_images:\n",
    "                    # 데이터 증강 적용\n",
    "                    cropped_image_augmented = data_augmentation(cropped_image)\n",
    "\n",
    "                    # 라벨 인코딩\n",
    "                    text, length = converter.encode([label], batch_max_length=opt.batch_max_length)\n",
    "\n",
    "                    # 모델 예측\n",
    "                    preds = str_model(cropped_image_augmented.unsqueeze(0).to(device))\n",
    "                    cost = criterion(preds, text)\n",
    "\n",
    "                    # 손실 역전파\n",
    "                    optimizer.zero_grad()\n",
    "                    cost.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss += cost.item()\n",
    "\n",
    "        if iteration % opt.valInterval == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {total_loss:.4f}\")\n",
    "            validate(opt)\n",
    "\n",
    "\n",
    "def validate(opt):\n",
    "    \"\"\"검증 루틴\"\"\"\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=True)\n",
    "    valid_dataset = TextRecognitionDataset(root=opt.valid_data, opt=opt)\n",
=======
    "def draw_boxes_on_image(image, boxes):\n",
    "    \"\"\"이미지에 탐지된 박스를 그림\"\"\"\n",
    "    image_with_boxes = image.copy()\n",
    "\n",
    "    for box in boxes:\n",
    "        poly = np.array(box).astype(np.int32).reshape((-1, 2))\n",
    "        cv2.polylines(image_with_boxes, [poly], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    return image_with_boxes\n",
    "\n",
    "def train(opt):\n",
    "    \"\"\"STR 모델 학습 루프\"\"\"\n",
    "    data_augmentation = DataAugmentation(opt.imgH, opt.imgW)  # 데이터 증강 객체 생성\n",
    "    \n",
    "    total_loss = 0\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    save_dir = \"./saved_models\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for iteration in range(opt.num_iter):\n",
    "        total_loss = 0\n",
    "        for img_file in os.listdir(opt.train_data):\n",
    "            if img_file.endswith('.jpg'):\n",
    "                image_path = os.path.join(opt.train_data, img_file)\n",
    "                label_path = os.path.splitext(image_path)[0] + \".json\"\n",
    "\n",
    "                try:\n",
    "                    image = imgproc.loadImage(image_path)\n",
    "                    label = load_label_json(label_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"이미지 불러오기 오류: {e} - 파일 경로: {image_path} 라벨 경로 : {label_path}\")\n",
    "                    continue\n",
    "\n",
    "                boxes, polys = detect_text_craft(image, craft_net, opt)  # 파라미터 전달\n",
    "\n",
    "                 # 원래 이미지에 박스를 그림\n",
    "                image_with_boxes = draw_boxes_on_image(image, boxes)\n",
    "\n",
    "                # OpenCV를 통해 이미지 시각화\n",
    "                cv2.imshow('Image with Boxes', image_with_boxes)\n",
    "                cv2.waitKey(0)  # 이미지 창이 닫힐 때까지 대기\n",
    "                cv2.destroyAllWindows()\n",
    "                \n",
    "                cropped_images = crop_text_regions(image, boxes)\n",
    "\n",
    "                if cropped_images is None or len(cropped_images) == 0:\n",
    "                    print(\"이미지 텐서가 비어 있습니다!\")\n",
    "                    continue\n",
    "\n",
    "                preds_str = []\n",
    "                for cropped_image in cropped_images:\n",
    "                    \n",
    "                    # numpy 배열이므로 PIL 이미지로 변환 후 흑백으로 처리\n",
    "                    if isinstance(cropped_image, np.ndarray):\n",
    "                        cropped_image = Image.fromarray(cropped_image).convert('L')\n",
    "                    cropped_image.show()\n",
    "                    cropped_image_augmented = data_augmentation(cropped_image)\n",
    "\n",
    "\n",
    "                    # 라벨 인코딩\n",
    "                    text, length = converter.encode([label], batch_max_length=opt.batch_max_length)\n",
    "\n",
    "                    # 모델 예측 (Attn 모델이므로 text[:, :-1] 사용)\n",
    "                    preds = str_model(cropped_image_augmented.unsqueeze(0).to(device), text[:, :-1])\n",
    "\n",
    "                    # 예측된 시퀀스의 인덱스 추출\n",
    "                    _, preds_index = preds.max(2)\n",
    "                    preds_str.append(converter.decode(preds_index, torch.IntTensor([preds.size(1)] * preds.size(0))))\n",
    "\n",
    "                combined_text = combine_text_predictions(preds_str)\n",
    "\n",
    "                # 라벨과 합친 텍스트 비교 및 출력\n",
    "                print(f\"Combined Prediction: {combined_text}, Ground Truth: {label}\")\n",
    "\n",
    "                # 타겟 설정 (라벨에서 [GO] 토큰을 제외한 텍스트)\n",
    "                target = text[:, 1:]  # [GO] 토큰 제외\n",
    "\n",
    "                # 예측된 시퀀스 길이를 타겟 길이에 맞춤\n",
    "                preds = preds[:, :target.size(1), :]\n",
    "\n",
    "                # 손실 계산 (CrossEntropyLoss로 계산)\n",
    "                cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "\n",
    "                # 손실 역전파 및 옵티마이저 업데이트\n",
    "                optimizer.zero_grad()\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += cost.item()\n",
    "\n",
    "        # 학습률 추적\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{iteration + 1}/{opt.num_iter}], Total Loss: {total_loss:.4f}, Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "        # 최적의 모델 가중치 저장\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            torch.save(str_model.state_dict(), f\"./saved_models/{opt.exp_name}_best.pth\")\n",
    "            print(f\"Best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "        print(f\"Model saved for epoch {iteration + 1}\")\n",
    "\n",
    "        # 검증 및 손실 수치 초기화\n",
    "        if (iteration + 1) % opt.valInterval == 0:\n",
    "            validate(opt)\n",
    "\n",
    "def validate(opt):\n",
    "    \"\"\"검증 루틴\"\"\"\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=True)\n",
    "    valid_dataset, _ = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=opt.workers, collate_fn=AlignCollate_valid)\n",
    "\n",
    "    str_model.eval()\n",
    "    with torch.no_grad():\n",
<<<<<<< HEAD
    "        valid_loss, valid_accuracy = validation(str_model, criterion, valid_loader, converter, opt)\n",
=======
    "        valid_loss, valid_accuracy = validation(str_model, criterion, valid_loader, converter, opt)[:2]\n",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
    "        print(f\"Validation Loss: {valid_loss:.4f}, Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "    str_model.train()\n",
    "\n",
    "\n",
    "# 트레이닝 실행\n",
    "train(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "4bee4481-9cd2-4f5d-a86d-a0461f0f819e",
=======
   "id": "91d9bc7e-5f98-4893-b09f-544ad23007d4",
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.4"
=======
   "version": "3.8.19"
>>>>>>> 691db04910b13dd3610a393f406ba95b5e1b20d9
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
