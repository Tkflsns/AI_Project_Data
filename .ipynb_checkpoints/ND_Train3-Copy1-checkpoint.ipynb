{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67310744-01b2-4c36-95a6-d8f546ceb813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images...\n",
      "Filtering the images containing characters which are not in opt.character\n",
      "Filtering the images whose label is longer than opt.batch_max_length\n",
      "--------------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------------\n",
      "num total samples of /: 239997 x 1.0 (total_data_usage_ratio) = 239997\n",
      "num samples of / per batch: 1500 x 1.0 (batch_ratio) = 1500\n",
      "--------------------------------------------------------------------------------\n",
      "Total_batch_size: 1500 = 1500\n",
      "--------------------------------------------------------------------------------\n",
      "Initializing TPS Transformation\n",
      "TPS 초기화: F=20, I_size=(32, 100), I_r_size=(32, 100), I_channel_num=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "D:\\KD7-3\\DataProject\\lincenseplateocr\\modules\\transformation.py:107: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
      "D:\\KD7-3\\DataProject\\lincenseplateocr\\modules\\transformation.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPS Transformation initialized\n",
      "Initializing Feature Extraction: ResNet\n",
      "Feature Extraction initialized with output size: 512\n",
      "Initializing Sequence Modeling with BiLSTM\n",
      "Sequence Modeling initialized\n",
      "Initializing Prediction: Attn\n",
      "Prediction initialized\n",
      "Model: Model(\n",
      "  (Transformation): TPS_SpatialTransformerNetwork(\n",
      "    (LocalizationNetwork): LocalizationNetwork(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (10): ReLU(inplace=True)\n",
      "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (14): ReLU(inplace=True)\n",
      "        (15): AdaptiveAvgPool2d(output_size=1)\n",
      "      )\n",
      "      (localization_fc1): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (localization_fc2): Linear(in_features=256, out_features=40, bias=True)\n",
      "    )\n",
      "    (GridGenerator): GridGenerator()\n",
      "  )\n",
      "  (FeatureExtraction): ResNet_FeatureExtractor(\n",
      "    (ConvNet): ResNet(\n",
      "      (conv0_1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn0_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv0_2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn0_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (layer2): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (maxpool3): MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
      "      (layer3): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (layer4): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (conv4_1): Conv2d(512, 512, kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), bias=False)\n",
      "      (bn4_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv4_2): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "      (bn4_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (AdaptiveAvgPool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
      "  (SequenceModeling): Sequential(\n",
      "    (0): BidirectionalLSTM(\n",
      "      (rnn): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): BidirectionalLSTM(\n",
      "      (rnn): LSTM(256, 256, batch_first=True, bidirectional=True)\n",
      "      (linear): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (Prediction): Attention(\n",
      "    (attention_cell): AttentionCell(\n",
      "      (i2h): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (score): Linear(in_features=256, out_features=1, bias=False)\n",
      "      (rnn): LSTMCell(404, 256)\n",
      "    )\n",
      "    (generator): Linear(in_features=256, out_features=148, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained model from ./lincenseplateocr/pretrained/Fine-Tuned.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_18292\\1784239689.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(opt.saved_model))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 루프 시작 at iteration 101\n",
      "검증 결과 - Loss: 0.3001, Accuracy: 69.77%\n",
      "[101/3000000] Train Loss: 2.0533, Valid Loss: 0.3001, Valid Accuracy: 69.7728\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 160. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 202\n",
      "검증 결과 - Loss: 0.1064, Accuracy: 86.74%\n",
      "[202/3000000] Train Loss: 0.0453, Valid Loss: 0.1064, Valid Accuracy: 86.7381\n",
      "검증 루프 시작 at iteration 303\n",
      "검증 결과 - Loss: 0.1879, Accuracy: 78.83%\n",
      "[303/3000000] Train Loss: 0.0721, Valid Loss: 0.1879, Valid Accuracy: 78.8309\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 321. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 404\n",
      "검증 결과 - Loss: 0.2255, Accuracy: 69.85%\n",
      "[404/3000000] Train Loss: 1.8583, Valid Loss: 0.2255, Valid Accuracy: 69.8529\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 482. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 505\n",
      "검증 결과 - Loss: 0.3813, Accuracy: 57.76%\n",
      "[505/3000000] Train Loss: 1.6116, Valid Loss: 0.3813, Valid Accuracy: 57.7620\n",
      "검증 루프 시작 at iteration 606\n",
      "검증 결과 - Loss: 0.1490, Accuracy: 79.26%\n",
      "[606/3000000] Train Loss: 1.3972, Valid Loss: 0.1490, Valid Accuracy: 79.2613\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 643. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 707\n",
      "검증 결과 - Loss: 0.1779, Accuracy: 77.01%\n",
      "[707/3000000] Train Loss: 1.6923, Valid Loss: 0.1779, Valid Accuracy: 77.0093\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 804. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 808\n",
      "검증 결과 - Loss: 0.1373, Accuracy: 82.26%\n",
      "[808/3000000] Train Loss: 0.0484, Valid Loss: 0.1373, Valid Accuracy: 82.2640\n",
      "검증 루프 시작 at iteration 909\n",
      "검증 결과 - Loss: 0.0638, Accuracy: 90.95%\n",
      "[909/3000000] Train Loss: 1.2798, Valid Loss: 0.0638, Valid Accuracy: 90.9519\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 965. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1010\n",
      "검증 결과 - Loss: 0.1340, Accuracy: 84.01%\n",
      "[1010/3000000] Train Loss: 0.0420, Valid Loss: 0.1340, Valid Accuracy: 84.0056\n",
      "검증 루프 시작 at iteration 1111\n",
      "검증 결과 - Loss: 0.3485, Accuracy: 64.80%\n",
      "[1111/3000000] Train Loss: 1.3612, Valid Loss: 0.3485, Valid Accuracy: 64.7983\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1126. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1212\n",
      "검증 결과 - Loss: 0.6881, Accuracy: 57.17%\n",
      "[1212/3000000] Train Loss: 1.8677, Valid Loss: 0.6881, Valid Accuracy: 57.1715\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1287. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1313\n",
      "검증 결과 - Loss: 0.1509, Accuracy: 82.98%\n",
      "[1313/3000000] Train Loss: 1.6632, Valid Loss: 0.1509, Valid Accuracy: 82.9847\n",
      "검증 루프 시작 at iteration 1414\n",
      "검증 결과 - Loss: 0.8423, Accuracy: 31.88%\n",
      "[1414/3000000] Train Loss: 1.2675, Valid Loss: 0.8423, Valid Accuracy: 31.8787\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1448. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1515\n",
      "검증 결과 - Loss: 0.2749, Accuracy: 72.59%\n",
      "[1515/3000000] Train Loss: 1.8165, Valid Loss: 0.2749, Valid Accuracy: 72.5853\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1609. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1616\n",
      "검증 결과 - Loss: 0.1679, Accuracy: 77.93%\n",
      "[1616/3000000] Train Loss: 1.4880, Valid Loss: 0.1679, Valid Accuracy: 77.9301\n",
      "검증 루프 시작 at iteration 1717\n",
      "검증 결과 - Loss: 0.2732, Accuracy: 70.96%\n",
      "[1717/3000000] Train Loss: 1.5801, Valid Loss: 0.2732, Valid Accuracy: 70.9639\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1770. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 1818\n",
      "검증 결과 - Loss: 1.1026, Accuracy: 33.33%\n",
      "[1818/3000000] Train Loss: 0.3922, Valid Loss: 1.1026, Valid Accuracy: 33.3300\n",
      "검증 루프 시작 at iteration 1919\n",
      "검증 결과 - Loss: 0.4881, Accuracy: 58.32%\n",
      "[1919/3000000] Train Loss: 0.4305, Valid Loss: 0.4881, Valid Accuracy: 58.3225\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 1931. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2020\n",
      "검증 결과 - Loss: 0.1260, Accuracy: 81.12%\n",
      "[2020/3000000] Train Loss: 0.4577, Valid Loss: 0.1260, Valid Accuracy: 81.1230\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2092. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2121\n",
      "검증 결과 - Loss: 0.3126, Accuracy: 67.32%\n",
      "[2121/3000000] Train Loss: 0.5808, Valid Loss: 0.3126, Valid Accuracy: 67.3206\n",
      "검증 루프 시작 at iteration 2222\n",
      "검증 결과 - Loss: 0.1932, Accuracy: 77.16%\n",
      "[2222/3000000] Train Loss: 0.2813, Valid Loss: 0.1932, Valid Accuracy: 77.1644\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2253. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2323\n",
      "검증 결과 - Loss: 0.5707, Accuracy: 48.99%\n",
      "[2323/3000000] Train Loss: 0.1216, Valid Loss: 0.5707, Valid Accuracy: 48.9941\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2414. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2424\n",
      "검증 결과 - Loss: 0.4623, Accuracy: 57.28%\n",
      "[2424/3000000] Train Loss: 1.6581, Valid Loss: 0.4623, Valid Accuracy: 57.2816\n",
      "검증 루프 시작 at iteration 2525\n",
      "검증 결과 - Loss: 0.0590, Accuracy: 91.61%\n",
      "[2525/3000000] Train Loss: 0.0237, Valid Loss: 0.0590, Valid Accuracy: 91.6125\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2575. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2626\n",
      "검증 결과 - Loss: 0.6459, Accuracy: 51.07%\n",
      "[2626/3000000] Train Loss: 0.0179, Valid Loss: 0.6459, Valid Accuracy: 51.0660\n",
      "검증 루프 시작 at iteration 2727\n",
      "검증 결과 - Loss: 0.4807, Accuracy: 57.25%\n",
      "[2727/3000000] Train Loss: 0.4097, Valid Loss: 0.4807, Valid Accuracy: 57.2515\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2736. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2828\n",
      "검증 결과 - Loss: 1.5736, Accuracy: 18.86%\n",
      "[2828/3000000] Train Loss: 1.4326, Valid Loss: 1.5736, Valid Accuracy: 18.8570\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 2897. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 2929\n",
      "검증 결과 - Loss: 2.3475, Accuracy: 14.83%\n",
      "[2929/3000000] Train Loss: 1.4281, Valid Loss: 2.3475, Valid Accuracy: 14.8334\n",
      "검증 루프 시작 at iteration 3030\n",
      "검증 결과 - Loss: 0.0850, Accuracy: 88.18%\n",
      "[3030/3000000] Train Loss: 0.0244, Valid Loss: 0.0850, Valid Accuracy: 88.1794\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 3058. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 3131\n",
      "검증 결과 - Loss: 0.7222, Accuracy: 46.31%\n",
      "[3131/3000000] Train Loss: 1.4916, Valid Loss: 0.7222, Valid Accuracy: 46.3117\n",
      "StopIteration 발생 at loader 0\n",
      "빈 배치가 감지되었습니다. 빈 배치를 건너뜁니다.\n",
      "빈 배치 발생 at iteration 3219. 배치를 건너뜁니다.\n",
      "검증 루프 시작 at iteration 3232\n",
      "검증 결과 - Loss: 0.3809, Accuracy: 66.93%\n",
      "[3232/3000000] Train Loss: 0.0148, Valid Loss: 0.3809, Valid Accuracy: 66.9302\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m opt\u001b[38;5;241m.\u001b[39mvalInterval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m101\u001b[39m  \u001b[38;5;66;03m# 검증 주기 설정\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 116\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(opt)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(opt\u001b[38;5;241m.\u001b[39mnum_iter):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         image_tensors, labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m image_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m빈 배치 발생 at iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. 배치를 건너뜁니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\KD7-3\\DataProject\\lincenseplateocr\\dataset.py:192\u001b[0m, in \u001b[0;36mBatch_Balanced_Dataset.get_batch\u001b[1;34m(self, drop_rate)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m     image, text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_loader_iter)\n\u001b[1;32m--> 192\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43maugmentor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 데이터 증강 적용\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     balanced_batch_images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m    194\u001b[0m     balanced_batch_texts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m text\n",
      "File \u001b[1;32mD:\\KD7-3\\DataProject\\lincenseplateocr\\dataset.py:120\u001b[0m, in \u001b[0;36mDataAugmentation.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"이미지에 변환 적용\"\"\"\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:540\u001b[0m, in \u001b[0;36mRandomApply.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 540\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:479\u001b[0m, in \u001b[0;36mLambda.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m--> 479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlambd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\KD7-3\\DataProject\\lincenseplateocr\\dataset.py:45\u001b[0m, in \u001b[0;36mDataAugmentation.add_noise.<locals>.noise\u001b[1;34m(batch_tensor_img)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     44\u001b[0m     tensor_img \u001b[38;5;241m=\u001b[39m batch_tensor_img[i]  \u001b[38;5;66;03m# 개별 이미지 추출\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     pil_img \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToPILImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_img\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m tensor_img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m transforms\u001b[38;5;241m.\u001b[39mToPILImage()(tensor_img)  \u001b[38;5;66;03m# 텐서 -> PIL 변환\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     np_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pil_img)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np_img\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:  \u001b[38;5;66;03m# 흑백 이미지일 경우\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\KD7-3\\lib\\site-packages\\torchvision\\transforms\\transforms.py:234\u001b[0m, in \u001b[0;36mToPILImage.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m        pic (Tensor or numpy.ndarray): Image to be converted to PIL Image.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pil_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "from lincenseplateocr.utils import CTCLabelConverter, CTCLabelConverterForBaiduWarpctc, AttnLabelConverter, Averager\n",
    "from lincenseplateocr.dataset import hierarchical_dataset, AlignCollate, Batch_Balanced_Dataset\n",
    "from lincenseplateocr.model import Model\n",
    "from lincenseplateocr.test import validation\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# 옵션 부분을 여기서 설정할 수 있게 변경\n",
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.exp_name = 'experiment'\n",
    "        self.train_data = './input/lmdb'\n",
    "        self.valid_data = './modules/Dataset/Valid'\n",
    "        self.manualSeed = 1111\n",
    "        self.workers = 0\n",
    "        self.batch_size = 1400\n",
    "        self.num_iter = 3000\n",
    "        self.valInterval = 20\n",
    "        self.saved_model = './lincenseplateocr/pretrained/Fine-Tuned.pth'\n",
    "        self.FT = True\n",
    "        self.adam = False\n",
    "        self.lr = 1.0\n",
    "        self.beta1 = 0.9\n",
    "        self.rho = 0.95\n",
    "        self.eps = 1e-8\n",
    "        self.grad_clip = 5\n",
    "        self.baiduCTC = False\n",
    "        self.select_data = '/'\n",
    "        self.batch_ratio = '1'\n",
    "        self.total_data_usage_ratio = '1.0'\n",
    "        self.batch_max_length = 25\n",
    "        self.imgH = 32\n",
    "        self.imgW = 100\n",
    "        self.rgb = False\n",
    "        self.character = '0123456789().JNRW_abcdef가강개걍거겅겨견결경계고과관광굥구금기김깅나남너노논누니다대댜더뎡도동두등디라러로루룰리마머명모무문므미바배뱌버베보부북비사산서성세셔소송수시아악안양어여연영오올용우울원육으을이익인자작저전제조종주중지차처천초추출충층카콜타파평포하허호홀후히ㅣ'\n",
    "        self.sensitive = False\n",
    "        self.PAD = False\n",
    "        self.Transformation = 'TPS'\n",
    "        self.FeatureExtraction = 'ResNet'\n",
    "        self.SequenceModeling = 'BiLSTM'\n",
    "        self.Prediction = 'Attn'\n",
    "        self.num_fiducial = 20\n",
    "        self.input_channel = 1\n",
    "        self.output_channel = 512\n",
    "        self.hidden_size = 256\n",
    "        self.num_gpu = torch.cuda.device_count()\n",
    "        self.data_filtering_off = False \n",
    "\n",
    "opt = Options()\n",
    "\n",
    "# 모델을 학습하는 함수\n",
    "def train(opt):\n",
    "    if not opt.data_filtering_off:\n",
    "        print('Filtering the images containing characters which are not in opt.character')\n",
    "        print('Filtering the images whose label is longer than opt.batch_max_length')\n",
    "\n",
    "    train_dataset = Batch_Balanced_Dataset(opt)\n",
    "    AlignCollate_valid = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    valid_dataset, valid_dataset_log = hierarchical_dataset(root=opt.valid_data, opt=opt)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=opt.batch_size, shuffle=True,\n",
    "        num_workers=int(opt.workers), collate_fn=AlignCollate_valid, pin_memory=True)\n",
    "    \n",
    "    # 모델 설정\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        if opt.baiduCTC:\n",
    "            converter = CTCLabelConverterForBaiduWarpctc(opt.character)\n",
    "        else:\n",
    "            converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    \n",
    "    print('Model:', model)\n",
    "    model = torch.nn.DataParallel(model).to(device)\n",
    "    model.train()\n",
    "\n",
    "    if opt.saved_model != '':\n",
    "        print(f'Loading pretrained model from {opt.saved_model}')\n",
    "        model.load_state_dict(torch.load(opt.saved_model))\n",
    "\n",
    "    # 손실 함수 및 옵티마이저 설정\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        criterion = torch.nn.CTCLoss(zero_infinity=True).to(device)\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
    "\n",
    "    if opt.adam:\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "    else:\n",
    "        optimizer = optim.Adadelta(filter(lambda p: p.requires_grad, model.parameters()), lr=opt.lr, rho=opt.rho, eps=opt.eps)\n",
    "\n",
    "    # 학습 루프\n",
    "    start_time = time.time()\n",
    "    for iteration in range(opt.num_iter):\n",
    "        try:\n",
    "            image_tensors, labels = train_dataset.get_batch()\n",
    "            if image_tensors is None or labels is None:\n",
    "                print(f\"빈 배치 발생 at iteration {iteration}. 배치를 건너뜁니다.\")\n",
    "                continue  # 빈 배치를 건너뛰기\n",
    "        except StopIteration:\n",
    "            print(f\"StopIteration 발생 at loader {iteration}\")\n",
    "            continue  # 배치가 끝났을 경우 다음 루프로 넘어감\n",
    "            \n",
    "        image = image_tensors.to(device)\n",
    "        text, length = converter.encode(labels, batch_max_length=opt.batch_max_length)\n",
    "        \n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model(image, text)\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * image.size(0))\n",
    "            preds = preds.log_softmax(2).permute(1, 0, 2)\n",
    "            cost = criterion(preds, text, preds_size, length)\n",
    "        else:\n",
    "            preds = model(image, text[:, :-1])\n",
    "            target = text[:, 1:]\n",
    "            cost = criterion(preds.view(-1, preds.shape[-1]), target.contiguous().view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        if (iteration + 1) % opt.valInterval == 0:\n",
    "            print(f\"검증 루프 시작 at iteration {iteration + 1}\")\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    valid_loss, valid_accuracy, *_ = validation(model, criterion, valid_loader, converter, opt)\n",
    "                    print(f\"검증 결과 - Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.2f}%\")\n",
    "                except StopIteration:\n",
    "                    print(f\"StopIteration 발생 at validation {iteration}\")\n",
    "                    continue\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"[{iteration+1}/{opt.num_iter}] Train Loss: {cost.item():.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}\")\n",
    "            \n",
    "        if (iteration + 1) % 10000 == 0:\n",
    "            torch.save(model.state_dict(), f\"./saved_models/{opt.exp_name}/iter_{iteration+1}.pth\")\n",
    "\n",
    "\n",
    "\n",
    "opt.exp_name = \"Number_Plate_Search\"\n",
    "opt.train_data = \"./lincenseplateocr/input/train\"\n",
    "opt.valid_data = \"./lincenseplateocr/input/Vali\"\n",
    "opt.num_iter = 500000  # 예시로 줄인 값\n",
    "opt.valInterval = 500  # 검증 주기 설정\n",
    "\n",
    "# 학습 실행\n",
    "train(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7a2ce-e0f5-4af3-8c86-a2a4d1dad5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# GPU 메모리 강제 수집\n",
    "gc.collect()  # Python 객체 수집 (CPU 메모리 해제)\n",
    "torch.cuda.empty_cache()  # GPU 메모리 캐시 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723ca3d-9a9f-413c-aa2d-b52eef7ea424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
